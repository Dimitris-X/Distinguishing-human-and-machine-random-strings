{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to investigate how we can differential human and LLM generated seemingly random sequences, from machine generated ones. Consider for instance a sequence of ones and 0s, each with an equal chance of occuring. While humans can accurately balance the freqeuncy of each kind, there is a tendency to avoid long sequences, and other possible biases that set these apart. This project attempts to find a method to capture this difference can be accuratly described and quantified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources for the data:\n",
    "The Randomly_generated.dat file was produced by myself using the numpy.random library.\n",
    "The Gpt_generated.dat file was made using the output of chatgpt o mini, when it was asked to produce a random sequence of 0s and 1s.\n",
    "Other strings of random symbols bellow where made by either chatgpt or myself, as explained in each instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis:\n",
    "By examining the information contained in each string, we can accurately describe its source.\n",
    "The methods used to do this include examining the entropy of a string and implementing a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: Entropy based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(length, symbols, *probabilities):\n",
    "    \"\"\"\n",
    "    generates a string of a given length, where each symbol in the sequence has the given probability of occuring at any spot\n",
    "    Note: for n symbols, only n-1 probabilites need to be provided\n",
    "    returns: string\n",
    "    This function will provide the standard for perfectly random generation\n",
    "    \"\"\"\n",
    "    throws = np.random.rand(length)\n",
    "    sequence = \"\"\n",
    "    for i, entry in enumerate(throws):\n",
    "        prob = 0\n",
    "        for j, prob_ in enumerate(probabilities):\n",
    "            prob += prob_\n",
    "            if entry<prob:\n",
    "                sequence+=symbols[j]\n",
    "                break\n",
    "        else:\n",
    "            sequence += symbols[-1]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(key, symbols, *probabilities):\n",
    "    \"\"\"\n",
    "    calculates the probability of a given string (key) occuring if it was generated according to the probabilities provided\n",
    "    Note: for n symbols, only n-1 probabilites need to be provided\n",
    "    returns: float\n",
    "    \"\"\"\n",
    "    prob = 1\n",
    "    last_prob = 1- sum(probabilities)\n",
    "    for i in key:\n",
    "        location_index = symbols.index(i)\n",
    "        if location_index == len(probabilities):\n",
    "            prob*=last_prob\n",
    "        else:\n",
    "            prob*=probabilities[location_index]\n",
    "    return 1/prob\n",
    "\n",
    "\n",
    "def convert_to_base(num, symbols):\n",
    "    \"\"\"\n",
    "    Converts a given number (num) to a base composed of the symbols provided.\n",
    "    For example if there are 6 symbols, the number will be prepresented in base 6, with 0 being the first symbol etc.\n",
    "    returns: string\n",
    "    \"\"\"\n",
    "    if num == 0:\n",
    "        return symbols[0]\n",
    "    \n",
    "    result = \"\"\n",
    "    while num > 0:\n",
    "        result = symbols[num % len(symbols)] + result\n",
    "        num //= len(symbols)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurance_chain(sequence, glyphs, symbols):\n",
    "    \"\"\"\n",
    "    Creates a dictionary, with key all possible length glyphs:int subsequences created with the symbols.\n",
    "    The value of each key is the percentage of times a subsequence occures in the sequence.\n",
    "    returns: dict \n",
    "    \"\"\"\n",
    "    dictionary = dict()\n",
    "    for i in range(len(symbols)**glyphs):\n",
    "        key  = convert_to_base(i, symbols)\n",
    "        key = key.rjust(glyphs, symbols[0])\n",
    "        dictionary.update({key:0})\n",
    "\n",
    "    for i in range(len(sequence)-glyphs+1):\n",
    "        dictionary[sequence[i:glyphs+i]]+=1\n",
    "\n",
    "    for key in dictionary.keys():\n",
    "        dictionary[key]/=(len(sequence)-glyphs+1)\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(dictionary, symbols, *probabilities):\n",
    "    \"\"\"\n",
    "    Caculates the cross entropy between the ideal distribution of a subsequence occuring formed by the probabilities provided,\n",
    "    and the disctribution derived from the sequence and provided in the form of a dictionary created by occurance_chain\n",
    "    returns: float\n",
    "    \"\"\"\n",
    "    entropy = 0\n",
    "    for key in dictionary.keys():\n",
    "        entropy -= np.log2(dictionary[key])*calculate_probability(key, symbols, *probabilities)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_for_n_glyphs(sequence, glyphs, symbols, *probabilites):\n",
    "    \"\"\"\n",
    "    Calculates the p value (the probability that the null hypothesis, that the string is tryly random, is false).\n",
    "    This is done by measuring the cross entropy of the given sequence, and randomly generating many others of equal length and measuring their entropy.\n",
    "    The p value is then the percentage of those randomly generated entropies that are equal or greater than the entropy of the sequence provided.\n",
    "    Note that a uniform prior is assumed.\n",
    "    returns: float\n",
    "    \"\"\"\n",
    "    cross_entropy = cross_entropy_loss(occurance_chain(sequence, glyphs, symbols), symbols, *probabilites)\n",
    "    # the lower bound determines how many random strings will be generated, thus limiting the accuracy with which p is calculated\n",
    "    # hence, a p value of 0 can only be interpered as something less the lower bound\n",
    "    lower_bound = 0.0001\n",
    "    number_of_tries = int(1/lower_bound)\n",
    "    number_of_rand = 0\n",
    "    for _ in range(number_of_tries):\n",
    "        rand_sequence = generate_sequence(len(sequence), symbols, *probabilites)\n",
    "        rand_cross_entorpy = cross_entropy_loss(occurance_chain(rand_sequence, glyphs, symbols), symbols, *probabilites)\n",
    "        if rand_cross_entorpy >=cross_entropy:\n",
    "            number_of_rand+=1\n",
    "    return number_of_rand/number_of_tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "The p value for the first string is 0.0091\n",
      "The p value for the second string is 0.0\n",
      "The p value for the third string is 0.3463\n",
      "The p value for the forth string is 0.2444\n"
     ]
    }
   ],
   "source": [
    "# testing the results so far\n",
    "\n",
    "# the first test involves \n",
    "# this string was generated by chagpt\n",
    "test_sequence_1 = \"ABBAAABABABBBABABABBAABBAABBBABABABBAABBABABABABABABAABBBABAABABBBABAABABBBAABABBBABBAAABAAABBAABBABABAAABBABBBAAABBBABABBAABBABABABABA\"\n",
    "print(len(test_sequence_1))\n",
    "\n",
    "# this string was generated by myself\n",
    "test_sequence_2 = \"ABABABABABBABBABABBABBABABBBAAABABABABAABABABBABABABBABABABBABABBABABAAABABABABABAAAABABABAABABBABABAAABBABBABAAABABAABABAABBABABAAABBB\"\n",
    "\n",
    "# this string was generated by myself, paying special attention in an attempt to\n",
    "# make it as random as possible in light of what I learned in this project\n",
    "test_sequence_3 = \"ABBAAABABABBBAAABBABAAAAABBAABAABBBBAAABABABAABAAABBBBABABBBABBBBAAAABABABBBABBABABABBBAAABABABABABABABABABBBBBAAAAABBAABABBABABBBBAAAB\"\n",
    "\n",
    "# this stirng is generated randomly through numpy\n",
    "test_sequence_4 = generate_sequence(100, [\"A\", \"B\"], 0.5)\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_1, 2, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the first string is {p}\")\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_2, 2, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the second string is {p}\")\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_3, 2, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the third string is {p}\")\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_4, 2, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the forth string is {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p value for the first string is 0.0054\n",
      "The p value for the second string is 0.0\n",
      "The p value for the third string is 0.3487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimit\\AppData\\Local\\Temp\\ipykernel_12460\\3507523467.py:9: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy -= np.log2(dictionary[key])*calculate_probability(key, symbols, *probabilities)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p value for the forth string is 0.3596\n"
     ]
    }
   ],
   "source": [
    "# the preceding test, used information of about the occurances of series of two symbols, the same can be done with more:\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_1, 3, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the first string is {p}\")\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_2, 3, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the second string is {p}\")\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_3, 3, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the third string is {p}\")\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_4, 3, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the forth string is {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimit\\AppData\\Local\\Temp\\ipykernel_12460\\3507523467.py:9: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy -= np.log2(dictionary[key])*calculate_probability(key, symbols, *probabilities)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p value for the third string is 0.1811\n"
     ]
    }
   ],
   "source": [
    "# for the third string, using a subsequence of length 4 is especially valuable, providing more certainty:\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_3, 4, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the third string is {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is important to remeber that increasing the number of symbols is not always helpful, as the number of possible combinations is of the form 2^n, and for large n the string becomes to small to build and accurate picture, as seen in the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimit\\AppData\\Local\\Temp\\ipykernel_12460\\3507523467.py:9: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy -= np.log2(dictionary[key])*calculate_probability(key, symbols, *probabilities)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p value for the first string is 0.0167\n"
     ]
    }
   ],
   "source": [
    "p = p_for_n_glyphs(test_sequence_1, 4, [\"A\", \"B\"], 0.5)\n",
    "print(f\"The p value for the first string is {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is not limited to two symbols, as seen below, however for accurate result, the more the sybmols, the greater the amount of data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimit\\AppData\\Local\\Temp\\ipykernel_12460\\3507523467.py:9: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy -= np.log2(dictionary[key])*calculate_probability(key, symbols, *probabilities)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p value is 0.0032\n"
     ]
    }
   ],
   "source": [
    "# the same but for three symbols instead of two\n",
    "\n",
    "# this string was generated by myself\n",
    "test_sequence_5 =\"ABCABABCABABCABBACAAABCBABACBABACBABCCAAACABCABABCABABCABBACAAABCBABACBABACBABCCAAAC\"\n",
    "\n",
    "p = p_for_n_glyphs(test_sequence_5, 2, [\"A\", \"B\", \"C\"], 0.33, 0.33)\n",
    "print(f\"The p value is {p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the entropy based method:\n",
    "This method can deduce whether a string is truly random or not mostly accurately.\n",
    "By calculing the p value associated with each sequence, we can somewhat reliably check if a sequence is truly random or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analysis: Neural network approach.\n",
    "In this section, as simple binary classifier neural network attemps to answer this question. The model will classify a series of fixed length 150 into either truly random (0) or not (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# loading the data from the two .dat folders, and splitting them into a test and train set\n",
    "X_gpt = []\n",
    "\n",
    "with open('Machine_Generated.dat', 'r') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    for row in reader:\n",
    "        X_gpt.append(np.array([float(i) for i in list(row[0])]))\n",
    "\n",
    "X_gpt = np.array(X_gpt)\n",
    "Y_gpt = np.ones(X_gpt.shape[0], dtype=\"float\")\n",
    "\n",
    "\n",
    "X_rand = []\n",
    "\n",
    "with open('Randomly_generated.dat', 'r') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    for row in reader:\n",
    "        X_rand.append(np.array([float(i) for i in list(row[0])]))\n",
    "\n",
    "X_rand = np.array(X_rand)\n",
    "Y_rand= np.zeros(X_rand.shape[0], dtype=\"float\")\n",
    "\n",
    "X = np.concatenate((X_rand, X_gpt))\n",
    "Y = np.concatenate((Y_rand, Y_gpt))\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple neural network that classifies a series into either truly random or biased\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(150, 200)  # Input layer to first hidden layer\n",
    "        self.layer2 = nn.Linear(200, 40)   # First hidden layer to second hidden layer\n",
    "        self.layer3 = nn.Linear(40, 1)     # Second hidden layer to output layer\n",
    "        self.relu = nn.ReLU()           # ReLU activation function\n",
    "        self.sigmoid = nn.Sigmoid()     # Sigmoid activation function for output between 1 and 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer and loss funcition initialization\n",
    "\n",
    "model = BinaryClassifier()\n",
    "\n",
    "loss_func = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.4252\n",
      "Epoch [20/200], Loss: 0.4190\n",
      "Epoch [30/200], Loss: 0.4127\n",
      "Epoch [40/200], Loss: 0.4000\n",
      "Epoch [50/200], Loss: 0.3813\n",
      "Epoch [60/200], Loss: 0.3543\n",
      "Epoch [70/200], Loss: 0.3181\n",
      "Epoch [80/200], Loss: 0.2787\n",
      "Epoch [90/200], Loss: 0.2429\n",
      "Epoch [100/200], Loss: 0.2078\n",
      "Epoch [110/200], Loss: 0.1703\n",
      "Epoch [120/200], Loss: 0.1282\n",
      "Epoch [130/200], Loss: 0.0858\n",
      "Epoch [140/200], Loss: 0.0515\n",
      "Epoch [150/200], Loss: 0.0285\n",
      "Epoch [160/200], Loss: 0.0161\n",
      "Epoch [170/200], Loss: 0.0099\n",
      "Epoch [180/200], Loss: 0.0067\n",
      "Epoch [190/200], Loss: 0.0049\n",
      "Epoch [200/200], Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    outputs = model(X_train_tensor) # calculates the output with the current state of the model\n",
    "    loss = loss_func(outputs, y_train_tensor) # calulates the loss with the respect to the last output\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Backpropagation to calculate gradients\n",
    "    optimizer.step()       # Update model weights\n",
    "    \n",
    "    # prints the loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8498\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "\n",
    "model.eval()  #evaluation mode\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = y_pred.round()  # Round the predictions to 0 or 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (y_pred_class.eq(y_test_tensor).sum().item()) / y_test_tensor.size(0)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = y_pred.round()  # Round the predictions to 0 or 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (y_pred_class.eq(y_test_tensor).sum().item()) / y_test_tensor.size(0)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate: 0.9333\n"
     ]
    }
   ],
   "source": [
    "# another metric for the evaluation is the false positive rate, comparable to the p value\n",
    "# for that, we will perform the test only with truly random sequences\n",
    "\n",
    "X_rand = []\n",
    "\n",
    "for _ in range(300):\n",
    "    X_rand.append(np.array([float(i) for i in list(generate_sequence(150, [\"0\", \"1\"], 0.5))]))\n",
    "\n",
    "X_rand = np.array(X_rand)\n",
    "Y_rand= np.zeros(X_rand.shape[0], dtype=\"float\")\n",
    "\n",
    "X_test_tensor = torch.tensor(X_rand, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(Y_rand, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = y_pred.round()  # Round the predictions to 0 or 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (y_pred_class.eq(y_test_tensor).sum().item()) / y_test_tensor.size(0)\n",
    "    print(f'False positive rate: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the neural network method:\n",
    "The model does detect a difference between the two types of sequences, and achived a respecable but low final accuracy of 85%. The false positive rate is about 7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "We can see that the hypothesis is indeed supported by the results obtained, as both methods managed to diffentiate between the two types of data, with varying degrees of accuracy. The entropy based method is more flexible and extendable but "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
